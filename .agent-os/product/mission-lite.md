# Qwen3-80B AutoRound Mission (Lite)

Qwen3-80B AutoRound is a local inference solution that enables localllama enthusiasts to run the Qwen3-Next-80B model on consumer hardware using Intel's 4-bit AutoRound quantization while waiting for llama.cpp integration.

This bridge solution serves the localllama community who need to run cutting-edge 80B models on consumer GPUs. By leveraging Intel's AutoRound quantization, we reduce memory requirements from 160GB to ~29GB while maintaining model quality and exposing chain-of-thought reasoning capabilities.